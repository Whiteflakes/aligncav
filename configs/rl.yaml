# RL Agent Training Configuration

# Environment settings
environment:
  max_steps: 100
  target_quality: 0.95
  image_size: 256
  num_motors: 4

# Cavity settings
cavity:
  mirror_reflectivity: 0.99
  cavity_length: 0.1  # meters
  wavelength: 1.064e-6  # 1064 nm
  input_waist: 100.0e-6  # 100 um

# Agent settings
agent:
  num_actions: 81  # 3^4 for 4 motors with {-1, 0, +1}
  learning_rate: 0.0001
  gamma: 0.99
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay: 10000
  target_update: 1000

# Replay buffer
replay_buffer:
  capacity: 100000
  min_size: 1000
  prioritized: false
  alpha: 0.6  # Priority exponent
  beta: 0.4   # Importance sampling

# Training settings
training:
  num_episodes: 1000
  batch_size: 64
  log_frequency: 10
  eval_frequency: 50
  eval_episodes: 10
  save_frequency: 100

# Reward settings
reward:
  base_reward: 0.01
  quality_weight: 0.99
  var_penalty_scale: 50.0
  success_bonus: 1.0

# Checkpointing
checkpoint:
  dir: "checkpoints"
  save_best: true

# Logging
logging:
  level: "INFO"
  tensorboard: false
  experiment_name: "rl_alignment"
